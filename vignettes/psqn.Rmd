---
title: "psqn: Partially Separable Quasi-Newton"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{psqn: Partially Separable Quasi-Newton}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: psqn.bib
---

<style>
img {
    border: none;
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, fig.align = "center", error = FALSE,
  comment = "#>", fig.width = 7, fig.height = 4)
options(digits = 3, width = 80)
```

$$\renewcommand\vec{\boldsymbol}   \def\bigO#1{\mathcal{O}(#1)}   \def\Cond#1#2{\left(#1\,\middle|\, #2\right)}   \def\mat#1{\boldsymbol{#1}}   \def\der{{\mathop{}\!\mathrm{d}}}   \def\argmax{\text{arg}\,\text{max}}  \def\Prob{\text{P}}  \def\diag{\text{diag}}   \def\argmin{\text{arg}\,\text{min}}   \def\Expe{\text{E}}$$

This package provides an optimization method for _partially separable_ 
functions. Partially separable functions are of the following form:

$$f(\vec x) = \sum_{i = 1}^n f_i(\vec x_{\mathcal I_i})$$

where $\vec x\in \mathbb R^l$,

$$\vec x_{\mathcal I_i} =    (\vec e_{j_{i1}}^\top, \dots ,\vec e_{j_{im_i}}^\top)\vec x, \qquad    \mathcal I_i = (j_{i1}, \dots, \mathcal j_{im_i}) \subseteq    \{1, \dots, l\}^l,$$
and $\vec e_k$ is the $k$'th column of the $l$ dimensional identity matrix. 
Each function $f_i$ is called an _element function_ and only depends on 
$m_i \ll l$ parameters. This allows for an efficient quasi-Newton 
method when all the $m_i$'s are much smaller than the dimension of the 
parameter vector $\vec x$, $l$. The framework can be extended to allow for 
a linear combination of parameters but we do not cover such problems. 
This vignette closely follows @nocedal06 who cover the methods and 
alternatives in much greater detail.

We only consider a more restricted form of the problem. Assume that each 
index set $\mathcal I_i$ is of the form:

$$\begin{align*}   \mathcal I_i &= \{1,\dots, p\} \cup \mathcal J_i \\   \mathcal J_i \cap \mathcal J_j &= \emptyset \qquad j\neq i \\   \mathcal J_i \cap \{1,\dots, p\} &= \emptyset \qquad \forall i = 1,\dots, n   \end{align*}.$$

That is, each index set contains $p$ _global parameters_ and 
$q_i = \lvert\mathcal J_i\rvert$ _private parameters_ which are particular 
for each element function, $f_i$. For implementation reason, we let:

$$\begin{align*}   \overleftarrow q_i &=    \begin{cases} p & i = 0 \\ p + \sum_{k = 1}^i q_k & i > 0 \end{cases} \\   \mathcal J_i &=    \{1 + \overleftarrow q_{i - 1}, \dots , q_i + \overleftarrow q_{i - 1}\}   \end{align*}$$

such that the element functions' private parameters lies in consecutive 
parts of $\vec x$. 

## Example
We are going to consider a Taylor approximation for a generalized linear 
mixed model. In particular, we focus on a mixed logit regression where: 

$$\begin{align*}   \vec U_i &\sim N^{(r)}(\vec 0, \mat\Sigma) \\   \vec\eta_i &= \mat X_i\vec\beta + \mat Z_i\vec U_i \\   Y_{ij} &\sim \text{Bin}(\text{logit}^{-1}(\eta_{ij}), 1),    \qquad j = 1, \dots, t_i   \end{align*}$$

where $N^{(r)}(\vec\mu,\mat\Sigma)$ means a $r$-dimensional a multivariate 
normal distribution with mean $\vec\mu$ and covariance matrix $\mat\Sigma$
and $\text{Bin}(p, k)$ means a binomial distribution probability $p$ and 
size $k$. $\vec U_i$ is an unknown random effect with an unknown covariance 
$\mat\Sigma$ and $\vec\beta\in\mathbb{R}^p$ are unknown fixed effect 
coefficients. $\mat X_i$ 
and $\mat Z_i$ are known design matrices each with $t_i$ rows for each of 
the $t_i$ observed outcomes, the $y_{ij}$s.

As part of a Taylor approximation, we find a mode of 
$\vec x = (\vec\beta^\top, \widehat{\vec u}_1^\top, \dots, \widehat{\vec u}_n^\top)$
of the log of the integrand given a covariance matrix estimate, 
$\widehat{\mat \Sigma}$. That is, we are minimizing:

$$\begin{align*}   f(\vec x) &= -\sum_{i = 1}^n \left(     \sum_{k = 1}^{t_i}(y_{ij}\eta_{ij} - \log(1 + \exp\eta_{ij}))     - \frac 12 \widehat{\vec u}_i^\top\widehat{\mat \Sigma}^{-1} \widehat{\vec u}_i     \right) \\   &= -\sum_{i = 1}^n \left(     \vec y_i(\mat X_i\vec\beta + \mat Z_i\widehat{\vec u}_i)     - \sum_{k = 1}^{t_i}     \log(1 + \exp(\vec x_{ik}^\top\vec\beta + \vec z_{ik}^\top\widehat{\vec u}_i))   - \frac 12 \widehat{\vec u}_i^\top\widehat{\mat \Sigma}^{-1} \widehat{\vec u}_i     \right) \\   &= \sum_{i = 1}^nf_i((\vec\beta^\top, \widehat{\vec u}_i^\top)^\top) \\   f_i((\vec\beta^\top, \vec u^\top)^\top) &=    -\vec y_i(\mat X_i\vec\beta + \mat Z_i\vec u)     + \sum_{k = 1}^{t_i}   \log(1 + \exp(\vec x_{ik}^\top\vec\beta + \vec z_{ik}^\top\vec u))   + \frac 12 \vec u^\top\widehat{\mat \Sigma}^{-1} \vec u   \end{align*}$$

In this problem, $\vec\beta$ are the global parameters and the 
$\widehat{\vec u}_i$'s are the private parameters. Thus, 
$l = p + nr$. We will later return to this example with an implementation 
which uses this package.

### Variational Approximations
The objective function for variational approximations for
mixed models for 
clustered data is commonly also partially separable. We will 
briefly summarize the idea here. @Ormerod12 and @Ormerod11
are examples where one might benefit from using the methods in this package.

We let $\tilde f_i$ be the log marginal likelihood term from cluster $i$. 
This is of the form:

$$
\tilde f_i(\vec\omega) = \log \int p_i(\vec y_i, \vec u;\vec\omega)\der \vec u 
$$

where $\vec\omega$ are unknown model parameters,
$p_i(\vec u;\vec\omega)$ is the joint density of the observed data denoted 
by $\vec y_i$, and $\vec U_i$ which is a 
cluster specific random effect. $\exp \tilde f_i(\vec\omega)$ is often 
intractable. An approximation of $\tilde f_i$ is to select some 
variational distribution denoted by $v_i$ parameterized by some set 
$\Theta_i$. We 
then use the approximation: 

$$
\begin{align*}
\tilde f_i(\vec\omega) &= \int v_i(\vec u; \vec\theta_i)
  \log\left(
  \frac{p_i(\vec y_i \vec u;\vec\omega)/v_i(\vec u; \vec\theta_i)}
       {p_i(\vec u_i \mid \vec y_i;\vec\omega)/v_i(\vec u; \vec\theta_i)}
  \right)\der\vec u \\
  &= 
  \int v_i(\vec u; \vec\theta_i)
  \log\left(
  \frac{p_i(\vec y_i, \vec u;\vec\omega)}
       {v_i(\vec u; \vec\theta_i)}
  \right)\der\vec u
  + \int v_i(\vec u; \vec\theta_i)
  \log\left(
  \frac{v_i(\vec u; \vec\theta_i)}
       {p_i(\vec u \mid \vec y_i;\vec\omega)}
  \right)\der\vec u \\
&\geq
  \int v_i(\vec u; \vec\theta_i)
  \log\left(
  \frac{p_i(\vec y_i, \vec u;\vec\omega)}
       {v_i(\vec u; \vec\theta_i)}
  \right)\der\vec u = f_i(\vec\omega,\vec\theta_i)
\end{align*}
$$

where $\vec\theta_i\in\Theta_i$ and
$p_i(\vec u_i \mid \vec y_i;\vec\omega)$ is the conditional density 
of the random effect given the observed data, $\vec y_i$, and model parameters, 
$\vec\omega$. $f_i(\vec\omega,\vec\theta_i)$ is a lower bound since the 
Kullback–Leibler divergence

$$
\int v_i(\vec u; \vec\theta_i)\log\left(
  \frac{v_i(\vec u; \vec\theta_i)}
       {p_i(\vec u \mid \vec y_i;\vec\omega)} 
  \right)\der\vec u
$$

is positive. The idea is to replace the minimization problem:

$$
\argmin_{\vec\omega} -\sum_{i = 1}^n \tilde f_i(\vec\omega)
$$

with a variational approximation:

$$
\argmin_{\vec\omega,\vec\theta_1,\dots,\vec\theta_n} 
  -\sum_{i = 1}^n f_i(\vec\omega,\vec\theta_i)
$$

This problem fits into the framework in the package where $\vec\omega$ 
are the global parameters and the $\vec\theta_i$s are the private parameters. 

Variational approximation have the property that if 
$v_i(\vec u; \vec\theta_i) = p_i(\vec u \mid \vec y_i;\vec\omega)$ then 
the Kullback–Leibler divergence is zero and the lower bound is equal 
to the log marginal likelihood. Thus, we need to use a family of 
variational distributions, $v_i$, which yields a close approximation 
of the conditional density of the random effects, 
$p_i(\vec u \mid \vec y_i;\vec\omega)$, for some 
$\vec\theta_i\in\Theta_i$. Moreover, the lower bound also needs to be 
easy to optimize. Variational approximations have an advantage that 
given estimates of 
$\widehat{\vec\omega},\widehat{\vec\theta}_1,\dots,\widehat{\vec\theta}_n$
then subsequent inference can be approximated using:

$$
\Expe\left(h(\vec U_i)\right) =
  \int h(\vec u) p_i(\vec u \mid \vec y_i;\vec\omega)\der\vec u 
  \approx 
  \int h(\vec u) v_i(\vec u; \widehat{\vec\theta}_i)\der\vec u.
$$

The latter integral may be much easier to work with for some functions 
$h$ and variational distribution, $v_i$. 

## Quasi-Newton Method for Partially Separable Functions
We are going to assume some prior knowledge of Newton's method and the 
Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm and we only provide a few 
details of these methods. However, will need a bit of notations from these 
methods to motivate the quasi-Newton method we have implemented. 

Newton's method to minimize a function is to start at some value 
$\vec x_0$. Then we set $k = 1$ and 

 1. compute a direction $\vec p_k$ given by $$\nabla^2 f(\vec x_{k - 1})\vec p_k = - \nabla f(\vec x_{k -1}),$$
 2. set $\vec x_k = \vec x_{k - 1} + \vec p_k$ or $\vec x_k = \vec x_{k - 1} + \gamma\vec p_k$ for $\gamma \in (0, 1]$ set
 to satisfy the [Wolfe conditions](https://en.wikipedia.org/wiki/Wolfe_conditions), and
 3. repeat with $k\leftarrow k + 1$ if a convergence criterion is not satisfied. 

Computing the Hessian, $\nabla^2 f(\vec x_{k - 1})$, at every iteration can be 
expensive. The BFGS algorithm offers an alternative where we use an 
approximation instead. Here we start with some Hessian approximation 
$\mat B_0$ and 

 1. compute a direction $\vec p_k$ given by $$\mat B_{k - 1}\vec p_k = - \nabla f(\vec x_{k -1}),$$
 2. find a step size $\alpha$ such that $\vec x_{k - 1} + \alpha\vec p_k$ satisfy 
 the [Wolfe conditions](https://en.wikipedia.org/wiki/Wolfe_conditions), 
 3. set $\vec x_k = \vec x_{k - 1} + \alpha\vec p_k$, 
 $\vec s_k = \alpha\vec p_k = \vec x_k - \vec x_{k - 1}$, 
 $\vec d_k = \nabla f(\vec x_k) - \nabla f(\vec x_{k - 1})$, 
 4. perform a rank-two update 
 $$\mat B_k = \mat B_{k - 1} + \frac{\vec y_k\vec y_k^\top}{\vec y_k^\top\vec s_k} - \frac{\mat B_{k - 1}\vec s_k\vec s_k^\top\mat B_{k - 1}^\top}{\vec s_k^\top\mat B_{k - 1}\vec s_k},$$ and 
 3. repeat with $k\leftarrow k + 1$ if a convergence criterion is not satisfied.
 
This reduces the cost of computing the Hessian. Further, we can update 
$\mat B_k^{-1}$ to avoid solving 
$\mat B_{k - 1}\vec p_k = - \nabla f(\vec x_{k -1})$. The matrix 
$\mat B_k^{-1}$ will still be large and dense when $l$ is large. 

### Using Partial Separability
As an alternative, we can exploit the structure of the problem we are 
solving. Let 

$$\mat H_i = (\vec e_{j_{i1}}^\top, \dots ,\vec e_{j_{im_i}}^\top).$$

The true Hessian in our case is sparse and given by 

$$\nabla^2 f(\vec x) = \sum_{i = 1}^n    \mat H_i^\top\nabla^2f_i(\vec x_{\mathcal I_i})\mat H_i$$
 
Notice that each $\nabla^2f_i(\vec x_{\mathcal I_i})$ is only a 
$(p + q_i)\times (p + q_i)$ matrix. We illustrate this below with $n = 10$
element functions. 
Each plot is $\mat H_i^\top\nabla^2f_i(\vec x_{\mathcal I_i})\mat H_i$ where
black entries are a non-zero.

```{r ex_hes_plot, echo = FALSE, fig.width = 7, fig.height = 2.8 * 1}
p <- 3L
q <- c(2L, 6L, 2L, 3L, 6L, 3L, 5L, 3L, 9L, 6L)

idx <- list()
off <- p
for(qi in q){
  idx <- c(idx, list(c(1:p, 1:qi + off)))
  off <- off + qi
}

l <- max(unlist(idx))
get_mat <- function(x){
  o <- matrix(0L, l, l)
  for(xj in x)
    for(xi in x)
      o[xi, xj] <- o[xj, xi] <- 1L
  o
}

img_rev <- function(x, ...){
  x <- x[, NROW(x):1]
  cl <- match.call()
  cl$x <- x
  cl[[1L]] <- image
  eval(cl, parent.frame())
}
par(mfcol = c(2, 5), mar = c(1, 1, 1, 1))
for(idxi in idx)
  img_rev(get_mat(idxi), xaxt = "n", yaxt = "n", 
          col = gray.colors(2L, 1, 0))
```

The whole Hessian is:

```{r whole_ex_hes_plot, echo = FALSE, fig.height = 3, fig.width = 3}
par(mar = c(.5, .5, .5, .5))
img_rev(apply(sapply(idx, get_mat, simplify = "array"), 1:2, sum) > 0, 
        xaxt = "n", yaxt = "n", col = gray.colors(2L, 1, 0))
```
 
We can use the partial separability to implement a BFGS method 
where we make $n$ BFGS approximations, one for each element function, 
$f_i$. Let $\mat B_{ki}$ be the approximation of 
$\nabla^2f_i(\vec x_{\mathcal I_i})$ at iteration $k$. Then the method 
we have implemented starts with $\mat B_{k1},\dots,\mat B_{kn}$ and 
 
  1. computes a direction $\vec p_k$ given by $$\left(\sum_{i = 1}^n\mat H_i^\top\mat B_{k - 1,i}\mat H_i\right)\vec p_k = - \nabla f(\vec x_{k -1}),$$
 2. finds a step size $\alpha$ such that $\vec x_{k - 1} + \alpha\vec p_k$ satisfy 
 the [Wolfe conditions](https://en.wikipedia.org/wiki/Wolfe_conditions), 
 3. sets $\vec x_k = \vec x_{k - 1} + \alpha\vec p_k$,
 4. performs BFGS updates for each $\mat B_{k1},\dots,\mat B_{kn}$, and 
 3. repeats with $k\leftarrow k + 1$ if a convergence criterion is not satisfied.

This seems as if it is going to be much slower as we are solving 
a large linear system if $l$ is large. 
However, we can use the conjugate gradient method we describe in the 
next section. This will be fast if we can perform the following 
matrix-vector product fast:

$$\left(\sum_{i = 1}^n\mat H_i^\top\mat B_{k - 1,i}\mat H_i\right)\vec z.$$

To elaborate on this, 
each $\mat H_i^\top\mat B_{k - 1,i}\mat H_i\vec z$ consists 
of matrix-vector product with a $o_i \times o_i$ symmetric 
matrix and a vector where $o_i = (p + q_i)$. This can be done in 
$2o_i(o_i + 1)$ flops. Thus, the total cost is 
$2\sum_{i = 1}^n o_i(o_i + 1)$ flops. This is in contrast to the 
original $2l(l + 1)$ flops with the BFGS method. 

As an example suppose that 
$q_i = 5$ for all $n$ element functions, 
$n = 5000$, and $p = 10$. Then $o_i = 15$ and the 
matrix-vector product above requires $2\cdot 5000 \cdot 15(15 + 1) = 2400000$ 
flops. In 
contrast $l = 5000 \cdot 5 + 10 = 25010$ and 
the matrix-vector product in the BFGS method 
requires $2\cdot 25010 (25010 + 1) =  1251050220$ flops. 
That is 521 times more flops. Similar ratios are shown in the
[BFGS and Partially Separable Quasi-Newton](#bfgs-and-partially-separable-quasi-newton)
section.

More formerly, the former is 
$\mathcal O(\sum_{i = 1}^n(p + q_{i})^2) = \mathcal O(np^2 + np\bar q + \sum_{i = 1}^nq_i^2)$ where 
$\bar q = \sum_{i = 1}^n q_i / n$ whereas the matrix-vector product 
in the BFGS method is 
$\mathcal O((p + n\bar q)^2) = \mathcal O(p^2 + pn\bar q + (n\bar q)^2)$. 
Thus, the former is favorable as long as $np^2 + \sum_{i = 1}^nq_i^2$ is 
small compared with $(n\bar q)^2$. 
Furthermore,
the rank-two BFGS updates are cheaper and may converge faster to a good 
approximation. However, we should keep in mind that 
the original BFGS method yields an approximation of $\mat B_k^{-1}$. Thus, 
we do not need to solve a linear system. However, we may not need to take 
many conjugate gradient iterations to get a good approximation with the 
implemented quasi-Newton method.

## Conjugate Gradient Method

The conjugate gradient method we use solves

$$\mat A\vec b = \vec v$$

which in our quasi-Newton method is 

$$\left(\sum_{i = 1}^n\mat H_i^\top\mat B_{k - 1,i}\mat H_i\right)\vec p_k = - \nabla f(\vec x_{k -1})$$

We start of with some initial value $\vec x_0$. Then we set $k = 0$, 
$\vec r_0 = \mat A\vec x_0 - \vec v$, $\vec p_0 = -\vec r_0$, and: 

1. find the step length $$\alpha_k = \frac{\vec r_k^\top\vec r_k}{\vec p_k^\top\mat A\vec p_k},$$ 
2. find the new value $$\vec x_{k + 1} = \vec x_k + \alpha_k\vec p_k,$$ 
3. find the new residual $$\vec r_{k + 1} = \vec r_k + \alpha_k\mat A\vec p_k,$$
4. set $\beta_{k + 1} = (\vec r_k^\top\vec r_k)^{-1}\vec r_{k + 1}^\top\vec r_{k + 1}$, 
5. set the new search direction to 
  $$\vec p_{k + 1} = - \vec r_{k + 1} + \beta_{k + 1}\vec p_k,$$
  and
6. stop if $\vec r_{k + 1}^\top\vec r_{k + 1}$ is smaller. Otherwise set $k\leftarrow k + 1$ and repeat.

The main issue is the matrix-vector product $\mat A\vec p_k$ but as we 
argued in the previous section that this can be computed in 
$\mathcal O(\sum_{i = 1}^n(p + q_{i})^2)$ time. The conjugate gradient method
will at most take $h$ iterations where $h$ is the number of rows and columns
of $\mat A$. Moreover, if $\mat A$ only has $r < h$ distinct eigenvalues 
then we will at most make $r$ conjugate gradient iterations. Lastly, if 
$\mat A$ has clusters of eigenvalues then we may expect to perform only 
a number of iterations close to the number of distinct clusters.

In practice, we terminate the conjugate gradient method when 
$\lVert\vec r_k\rVert < \min (c, \sqrt{\lVert\nabla f(\vec x_{k -1})\rVert})\lVert\nabla f(\vec x_{k -1})\rVert$
where $c$ is a constant the user can set. Moreover, we use diagonal 
preconditioning.

## Line Search and Wolfe Condition
We use line search and search for a point which satisfy
[the strong Wolfe condition](https://en.wikipedia.org/wiki/Wolfe_conditions)
by default. 
The constants in the Wolfe condition can be set by the user. The line search 
is implemented as described by @nocedal06 with cubic interpolation in the 
zoom phase.

## Symmetric Rank-one Updates
[Symmetric rank-one (SR1)](https://en.wikipedia.org/wiki/Symmetric_rank-one)
updates are implemented as an alternative to the BFGS updates. 
The user can set whether the SR1 updates should be used. The SR1 updates 
does not guarantee that the Hessian approximation is positive definite. 
Thus, the conjugate gradient method only proceeds if 
$\vec p_k^\top\mat A\vec p_k > 0$. That is, if the new direction is a 
descent direction.

## Example Using the Implementation

We simulate a data set below from the mixed logit model we showed earlier.

```{r sim_dat}
# assign model parameters, number of random effects, and fixed effects
q <- 4 # number of private parameters per cluster
p <- 5 # number of global parameters
beta <- sqrt((1:p) / sum(1:p))
Sigma <- diag(q)

# simulate a data set
n_clusters <- 800L # number of clusters
set.seed(66608927)

sim_dat <- replicate(n_clusters, {
  n_members <- sample.int(20L, 1L) + 2L
  X <- matrix(runif(p * n_members, -sqrt(6 / 2), sqrt(6 / 2)), 
              p)
  u <- drop(rnorm(q) %*% chol(Sigma))
  Z <- matrix(runif(q * n_members, -sqrt(6 / 2 / q), sqrt(6 / 2 / q)), 
              q)
  eta <- drop(beta %*% X + u %*% Z)
  y <- as.numeric((1 + exp(-eta))^(-1) > runif(n_members))
  
  list(X = X, Z = Z, y = y, u = u, Sigma_inv = solve(Sigma))
}, simplify = FALSE)

# example of the first cluster
sim_dat[[1L]]
```

The combined vector with global and private parameters can be created like 
this (it is a misnoma to call this `true_params` as the modes of the random 
effects, the private parameters, should only match the random effects if 
the clusters are very large): 

```{r show_glob_priv}
true_params <- c(beta, sapply(sim_dat, function(x) x$u))

# global parameters
true_params[1:p]

# some of the private parameters
true_params[1:(4 * q) + p]
```

As a reference, we will create the following function to evaluate the 
log of the integrand: 

```{r assing_integrand}
eval_integrand <- function(par){
  out <- 0.
  inc <- p
  beta <- par[1:p]
  for(i in seq_along(sim_dat)){
    dat <- sim_dat[[i]]
    X <- dat$X
    Z <- dat$Z
    y <- dat$y
    Sigma_inv <- dat$Sigma_inv
    
    u <- par[1:q + inc]
    inc <- inc + q
    eta <- drop(beta %*% X + u %*% Z)
    
    out <- out - drop(y %*% eta) + sum(log(1 + exp(eta))) + 
      .5 * drop(u %*% Sigma_inv %*% u)
  }
  
  out
}

# check the log integrand at true global parameters and the random effects
eval_integrand(true_params)
```

We will use this function to compare with our C++ implementation.

### R Implementation

A R function which we need to pass to `psqn` to minimize the partially 
separable function is given below:

```{r assing_r_api}
# evalutes the negative log integrand. 
# 
# Args:
#   i cluster/element function index. 
#   par the global and private parameter for this cluster. It has length 
#       zero if the number of parameters is requested.
#   comp_grad logical for whether to compute the gradient. 
r_func <- function(i, par, comp_grad){
  dat <- sim_dat[[i]]
  X <- dat$X
  Z <- dat$Z
  
  if(length(par) < 1)
    # requested the dimension of the parameter
    return(c(global_dim = NROW(dat$X), private_dim = NROW(dat$Z)))
  
  y <- dat$y
  Sigma_inv <- dat$Sigma_inv
  
  beta <- par[1:p]
  uhat <- par[1:q + p]
  eta <- drop(beta %*% X + uhat %*% Z)
  exp_eta <- exp(eta)
  
  out <- -sum(y * eta) + sum(log(1 + exp_eta)) + 
    sum(uhat * (Sigma_inv %*% uhat)) / 2
  if(comp_grad){
    d_eta <- -y + exp_eta / (1 + exp_eta)
    grad <- c(X %*% d_eta, 
              Z %*% d_eta + dat$Sigma_inv %*% uhat)
    attr(out, "grad") <- grad
  }
  
  out
}
```

Here is a check that the above yields the same as the function we defined 
before:

```{r check_assing_r_api}
# check the function
r_func_val <- sum(sapply(1:n_clusters, function(i)
  r_func(i, true_params[c(1:p, 1:q + (i - 1L) * q + p)], FALSE)))
all.equal(eval_integrand(true_params), r_func_val)

# we could check the gradient like this
if(FALSE){
  r_func_gr <- numeric(length(true_params))
  for(i in seq_along(sim_dat)){
    out_i <- r_func(i, true_params[c(1:p, 1:q + (i - 1L) * q + p)], TRUE)
    r_func_gr[1:p] <- r_func_gr[1:p] + attr(out_i, "grad")[1:p]
    r_func_gr[1:q + (i - 1L) * q + p] <- attr(out_i, "grad")[1:q + p]
  }
  
  library(numDeriv)
  gr_num <- grad(function(par) eval_integrand(par), true_params)
  all.equal(r_func_gr, gr_num, tolerance = 1e-6)
}
```

The partially separable function can be minimized like this:

```{r min_with_R, cache = 1}
start_val <- true_params 
start_val[  1:p ] <- 
  start_val[  1:p ] + 
  c(0.49, -0.63, -0.4, -0.33, -0.38) # ~rnorm(length(beta), sd = .5)
start_val[-(1:p)] <- 0

library(psqn)
r_psqn_func <- function(par, n_threads = 2L, c1 = 1e-4, 
                        c2 = .9)
  psqn(par = par, fn = r_func, n_ele_func = n_clusters, 
       n_threads = n_threads, c1 = c1, c2 = c2)

R_res <- r_psqn_func(start_val)
```

We will later compare this with the result from the C++ implementation 
which we provide in the next section.

### C++ Implementation
We provide a C++ implementation with the package as an example of how to use 
this package. The location of the implementation can be found by 
calling `system.file("mlogit-ex.cpp", package = "psqn")`. 
The most important part of the implementation is the problem specific 
`m_logit_func` class, 
the `get_mlogit_optimizer` function and the `optim_mlogit` function which 
are needed to perform the optimization. The content of the file is:

```{cpp mlogit_cpp_code, code = readLines(system.file("mlogit-ex.cpp", package = "psqn"))}
```

The `PSQN::R_reporter` class ensures that output will be printed when one 
passes a `trace` argument which is greater than zero. The 
`PSQN::R_interrupter` class ensures that the user can interrupt the 
computation. These two classes can be replaced with custom classes  
if one wants to and provide another implementation. See the source code 
of this package for the required members.

We can use the code by calling `Rcpp::sourceCpp` to compile the code: 

```{r create_ptr, cache = 1}
library(Rcpp)
sourceCpp(system.file("mlogit-ex.cpp", package = "psqn"))
```

Then we can create a pointer to an optimizer and check that it yields the 
correct value and gradient like this:

```{r get_optimizer, cache = 1, dependson = "create_ptr"}
optimizer <- get_mlogit_optimizer(sim_dat, max_threads = 4L)

stopifnot(all.equal(
  eval_integrand(true_params), 
  eval_mlogit(val = true_params, ptr = optimizer, n_threads = 2L)))

library(numDeriv)
gr_num <- grad(
  function(par) eval_mlogit(val = par, ptr = optimizer, n_threads = 2L), 
  true_params)
gr_opt <- grad_mlogit(val = true_params, ptr = optimizer, n_threads = 2L)

stopifnot(all.equal(gr_num, gr_opt, tolerance = 1e-5, 
                    check.attributes = FALSE),
          # also check the function value!
          all.equal(attr(gr_opt, "value"), 
                    eval_mlogit(val = true_params, ptr = optimizer, 
                                n_threads = 2L)))
```

We can now use the BFGS implementation in the `optim` function to 
compare with like this:

```{r comp_optim, cache = 1, dependson = "create_ptr"}
optim_func <- function(par, n_threads = 2L)
  optim(
    par, function(par) 
      eval_mlogit(val = par, ptr = optimizer, n_threads = n_threads), 
    function(par) 
      grad_mlogit(val = par, ptr = optimizer, n_threads = n_threads), 
    method = "BFGS", control = list(reltol = 1e-8))

bfgs_res <- optim_func(start_val)
```

We then use the quasi-Newton method like this:

```{r use_qn, cache = 1, dependson = "create_ptr"}
psqn_func <- function(par, n_threads = 2L, c1 = 1e-4, 
                      c2 = .9, trace = 0L, use_bfgs = TRUE, 
                      opt_private = FALSE){
  rel_eps <- 1e-8
  if(opt_private){
    # it may be useful to fix the global parameters and optimize the 
    # private parameters to get starting values. This is very fast as each 
    # set of parameters can be optimized separately
    par <- optim_mlogit_private(
      val = par, ptr = optimizer, rel_eps = sqrt(rel_eps), max_it = 100, 
      n_threads = n_threads, c1 = c1, c2 = c2)
  }
  optim_mlogit(val = par, ptr = optimizer, rel_eps = rel_eps, 
               max_it = 1000L, n_threads = n_threads, c1 = c1, 
               c2 = c2, trace = trace, use_bfgs = use_bfgs)
}

psqn_res <- psqn_func(start_val)

# using SR1 updates
psqn_res_sr1 <- psqn_func(start_val, use_bfgs = FALSE)
all.equal(psqn_res_sr1$value, psqn_res$value)

# w/ different starting values
psqn_res_diff_start <- psqn_func(start_val, opt_private = TRUE)
all.equal(psqn_res$value, psqn_res_diff_start$value)
```

The `counts` element contains the number of function evaluations, 
gradient evaluations, and the total number of conjugate gradient 
iterations:

```{r show_counts}
psqn_res$counts

# it is the same as we got from R
all.equal(psqn_res$par, R_res$par)
all.equal(psqn_res$value, R_res$value)

# compare with optim
bfgs_res$counts
```

We can compare the solution with the solution from `optim`:

```{r comp_solution}
all.equal(bfgs_res$par, psqn_res$par)
all.equal(psqn_res$value, bfgs_res$value, tolerance = 1e-8)
psqn_res$value - bfgs_res$value
```

The `optim_mlogit` takes fewer iterations possibly because we quicker get
a good approximation of the Hessian. Furthermore, we only take 
`psqn_res$counts["n_cg"]`, `r psqn_res$counts["n_cg"]`, 
 conjugate gradient iterations. This in contrast to the worst 
case scenario where we make `length(start_val)`, 
`r length(start_val)`, iterations for just
one iteration of the quasi-Newton method! We can also compare with 
the limited memory BFGS minimizer from the `lbfgsb3c` package:

```{r comp_with_lbfgsb3c, cache = 1, dependson = "create_ptr"}
library(lbfgsb3c)
lbfgsb3c_func <- function(par, n_threads = 2L)
  lbfgsb3c(par = par, function(par) 
      eval_mlogit(val = par, ptr = optimizer, n_threads = n_threads), 
    function(par) 
      grad_mlogit(val = par, ptr = optimizer, n_threads = n_threads), 
    control = list(factr = 1e-8 * 10, maxit = 1000L))

lbfgsb3c_res <- lbfgsb3c_func(start_val)

all.equal(lbfgsb3c_res$par, psqn_res$par)
all.equal(lbfgsb3c_res$value, bfgs_res$value)
psqn_res$value - lbfgsb3c_res$value
```

We can also compare with the limited memory BFGS minimizer from the 
`lbfgs` package:

```{r comp_with_liblbfgs, cache = 1, dependson = "create_ptr"}
library(lbfgs)
lbfgs_func <- function(par, n_threads = 2L)
  lbfgs(vars = par, function(par) 
      eval_mlogit(val = par, ptr = optimizer, n_threads = n_threads), 
    function(par) 
      grad_mlogit(val = par, ptr = optimizer, n_threads = n_threads), 
    invisible = 1)

lbfgs_res <- lbfgs_func(start_val)

all.equal(lbfgs_res$par, psqn_res$par)
all.equal(lbfgs_res$value, bfgs_res$value)
psqn_res$value - lbfgs_res$value
```

We can get the Hessian approximation by calling the `get_Hess_approx_mlogit`
function we declared after calling the optimizer: 

```{r get_hess_ex, cache = 1, dependson = "get_optimizer"}
aprox_hes <- get_Hess_approx_mlogit(ptr = optimizer)
dim(aprox_hes) # quite large; requires a lot of memory

# create a plot like before. Black entries are non-zero
par(mar = c(.5, .5, .5, .5))
idx <- 1:min(1000, NROW(aprox_hes))
aprox_hes <- aprox_hes[idx, idx] # reduce dimension to plot quickly
image(abs(aprox_hes[, NCOL(aprox_hes):1]) > 0, xaxt = "n", yaxt = "n",
      col = gray.colors(2L, 1, 0))
if(FALSE){
  # only feasible for smaller problem
  hess_true <- jacobian(
    function(par) grad_mlogit(val = par, ptr = optimizer), 
    psqn_res$par)
  
  # should not hold exactly! Might not be that good of an approximation.
  all.equal(aprox_hes, hess_true) 
}
```

The true Hessian is very sparse. Finally, here is a benchmark to compare the
computation time:

```{r end_bench, cache = 1, dependson = "create_ptr"}
bench::mark(
  `   optim BFGS (2 threads)`  = optim_func  (start_val, n_threads = 2L),
  `         lbfgs (1 thread)` = lbfgs_func   (start_val, n_threads = 1L),
  `         lbfgs(2 threads)` = lbfgs_func   (start_val, n_threads = 2L),
  `         lbfgs(4 threads)` = lbfgs_func   (start_val, n_threads = 4L),
  `      lbfgsb3c (1 thread)` = lbfgsb3c_func(start_val, n_threads = 1L),
  `      lbfgsb3c(2 threads)` = lbfgsb3c_func(start_val, n_threads = 2L),
  `      lbfgsb3c(4 threads)` = lbfgsb3c_func(start_val, n_threads = 4L),
  `       psqn (R; 1 thread)` = r_psqn_func  (start_val, n_threads = 1L),
  `       psqn(R; 2 threads)` = r_psqn_func  (start_val, n_threads = 2L),
  `     psqn (1 thread, SR1)` = psqn_func    (start_val, n_threads = 1L, 
                                              use_bfgs = FALSE),
  `     psqn(2 threads, SR1)` = psqn_func    (start_val, n_threads = 2L, 
                                              use_bfgs = FALSE),
  `psqn  (1 thread, opt pri.)` = psqn_func   (start_val, n_threads = 1L, 
                                              opt_private = TRUE),
  `psqn (2 threads, opt pri.)` = psqn_func   (start_val, n_threads = 2L, 
                                              opt_private = TRUE),
  `          psqn (1 thread)` = psqn_func    (start_val, n_threads = 1L),
  `          psqn(2 threads)` = psqn_func    (start_val, n_threads = 2L),
  `          psqn(4 threads)` = psqn_func    (start_val, n_threads = 4L),
  check = FALSE, min_time = 5)
```

We see a large reduction. To be fair, we can use the C interface 
for the limited-memory BFGS methods to avoid re-allocating the gradient
at every iteration. This will reduce their computation time. The R version of
the quasi-Newton method is slower mainly as the R version to evaluate the 
log of the integrand and its derivative is slower than the version used by 
all the other methods. We can illustrate this by comparing with the 
computation time with the `eval_integrand`:

```{r integrand_time, cache = 1}
bench::mark(
  `  R` = eval_integrand(true_params), 
  `C++` = eval_mlogit(val = true_params, ptr = optimizer, n_threads = 1L), 
  min_iterations = 100)
```

There is a big difference. Moreover, there is an overhead with 
repeatedly going back and forward between R and C++. A fair comparison would
use an R implementation for all methods.

### Polynomial Example

We consider the following trivial (regression) example as there is an 
explicit solution to compare with:

$$
\begin{align*}
\mathcal G &=\{1,\dots, p\} \\
\mathcal G  \cap \mathcal P_i &= \emptyset \\
\mathcal P_j \cap \mathcal P_i &= \emptyset, \qquad i\neq j \\
\mathcal I_i &\in \{1,\dots, p\}^{\lvert\mathcal P_i\rvert} \\
f(\vec x) &= (\vec x_{\mathcal G} - \vec\mu_{\mathcal G})^\top
  (\vec x_{\mathcal G} - \vec\mu_{\mathcal G}) +
  \sum_{i = 1}^n 
  (\vec x_{\mathcal P_i} - \vec\mu_{\mathcal P_i} - 
   \mat\Psi_i\vec x_{\mathcal I_i})^\top
  (\vec x_{\mathcal P_i} - \vec\mu_{\mathcal P_i} - 
   \mat\Psi_i\vec x_{\mathcal I_i}) 
\end{align*}
$$
This is not because the problem is interesting per se but it is meant as 
another illustration. R code to simulate from this model is given below:

```{r poly_ex}
# simulate the data
set.seed(1)
n_global <- 10L
n_clusters <- 50L

mu_global <- rnorm(n_global)
idx_start <- n_global

cluster_dat <- replicate(n_clusters, {
  n_members <- sample.int(n_global, 1L)
  g_idx <- sort(sample.int(n_global, n_members))
  mu_cluster <- rnorm(n_members)
  Psi <- matrix(rnorm(n_members * n_members), n_members, n_members)
  
  out <- list(idx = idx_start + 1:n_members, g_idx = g_idx,
              mu_cluster = mu_cluster, Psi = Psi)
  idx_start <<- idx_start + n_members
  out
}, simplify = FALSE)

# assign matrices needed for comparisons
library(Matrix)
M <- diag(idx_start)
for(cl in cluster_dat)
  M[cl$idx, cl$g_idx] <- -cl$Psi
M <- as(M, "dgCMatrix")

# Assign two R functions to evaluate the objective function. There are two 
# versions of the function to show that we get the same with one being 
# closer to the shown equation
fn_one <- function(par, ...){
  delta <- par[1:n_global] - mu_global
  out <- drop(delta %*% delta)
  for(cl in cluster_dat){
    delta <- drop(par[cl$idx] - cl$mu_cluster - cl$Psi %*% par[cl$g_idx])
    out <- out + drop(delta %*% delta)
  }
  out
}
fn_two <- function(par, ...){
  mu <- c(mu_global, unlist(sapply(cluster_dat, "[[", "mu_cluster")))
  delta <- drop(M %*% par - mu)
  drop(delta %*% delta)
}

tmp <- rnorm(idx_start)
all.equal(fn_one(tmp), fn_two(tmp)) # we get the same w/ the two
fn <- fn_two
rm(fn_one, fn_two, tmp)

# assign gradient function
gr <- function(par, ...){
  mu <- c(mu_global, unlist(sapply(cluster_dat, "[[", "mu_cluster")))
  2 * drop(crossprod(M, drop(M %*% par - mu)))
}

# we can easily find the explicit solution
mu <- c(mu_global, unlist(sapply(cluster_dat, "[[", "mu_cluster")))
exp_res <- drop(solve(M, mu))
fn(exp_res) # ~ zero as it should be
```

C++ code to work with this function is provided at 
`system.file("poly-ex.cpp", package = "psqn")` with the package
and given below:

```{cpp poly_cpp_code, code = readLines(system.file("poly-ex.cpp", package = "psqn"))}
```

We can `Rcpp::sourceCpp` the file and use the code like below to find the
solution:

```{r cpp_w_poly}
library(Rcpp)
sourceCpp(system.file("poly-ex.cpp", package = "psqn"))

# get a pointer to C++ object
optimizer <- get_poly_optimizer(cluster_dat, mu_global = mu_global, 
                                max_threads = 2L)

# we get the same function value and gradient
tmp <- rnorm(idx_start)
all.equal(fn       (tmp), 
          eval_poly(tmp, optimizer, 1L))
all.equal(gr       (tmp), 
          grad_poly(tmp, optimizer, 1L), 
          check.attributes = FALSE)

# run the optimization
psqn_func <- function(par, n_threads = 2L, c1 = 1e-4, 
                      c2 = .9, trace = 0L)
  optim_poly(val = par, ptr = optimizer, rel_eps = 1e-8, max_it = 1000L, 
             n_threads = n_threads, c1 = c1, 
             c2 = c2, trace = trace)

psqn_res <- psqn_func(numeric(idx_start))
all.equal(exp_res, psqn_res$par)
```

A version using the R function `psqn` is:

```{r r_w_poly}
# assign function to pass to psqn
r_func <- function(i, par, comp_grad){
  dat <- cluster_dat[[i]]
  g_idx <- dat$g_idx
  mu_cluster <- dat$mu_cluster
  Psi <- dat$Psi
  
  if(length(par) < 1)
    # requested the dimension of the parameter
    return(c(global_dim = length(mu_global), 
             private_dim = length(mu_cluster)))
  
  is_glob <- 1:length(mu_global)
  x_glob <- par[is_glob]
  x_priv <- par[-is_glob]
  
  delta <- drop(x_priv - Psi %*% x_glob[g_idx] - mu_cluster)
  
  out <- drop(delta %*% delta)
  if(i == 1L){
    delta_glob <- x_glob - mu_global
    out <- out + drop(delta_glob %*% delta_glob)
  }
  
  if(comp_grad){
    grad <- numeric(length(mu_cluster) + length(mu_global))
    grad[g_idx] <- -2 * drop(crossprod(Psi, delta))
    grad[-is_glob] <- 2 * delta
    if(i == 1L)
      grad[is_glob] <- grad[is_glob] + 2 * delta_glob
    attr(out, "grad") <- grad
  }
  
  out
}

# use the function
r_psqn_func <- function(par, n_threads = 2L, c1 = 1e-4, 
                        c2 = .9, trace = 0L)
  psqn(par = par, fn = r_func, n_ele_func = n_clusters, 
       n_threads = n_threads, c1 = c1, c2 = c2, 
       trace = trace, max_it = 1000L)

R_res <- r_psqn_func(numeric(idx_start))
all.equal(exp_res, R_res$par)
```

## Details
### Using the Code in a Package
The main part of this packages is a header-only library. Thus, the code can 
be used within a R package by adding `psqn` to `LinkingTo` in the 
DESCRIPTION file. This is an advantage as one can avoid repeated compilation
of the code. 

Moreover, since the main part of the code is a header-only 
library, this package can easily be used within languages which can 
easily call C++ code.

### BFGS Method
There is also a BFGS implementation in the package. This is both available 
in R through the `psqn_bfgs` function and in C++ in the psqn-bfgs.h 
header file. An example is provided below using the example from `optim`:

```{r bfgs_example}
# declare function and gradient from the example from help(optim)
fn <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
gr <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
     200 *      (x2 - x1 * x1))
}

# we need a different function for the method in this package
gr_psqn <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  out <- c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
            200 *      (x2 - x1 * x1))
  attr(out, "value") <- 100 * (x2 - x1 * x1)^2 + (1 - x1)^2
  out
}

# we get the same
optim    (c(-1.2, 1), fn, gr, method = "BFGS")$par
psqn_bfgs(c(-1.2, 1), fn, gr_psqn)            $par

# they run in about the same time
bench::mark(
  optim     = optim    (c(-1.2, 1), fn, gr, method = "BFGS"), 
  psqn_bfgs = psqn_bfgs(c(-1.2, 1), fn, gr_psqn), 
  check = FALSE, min_time = .5)
```

### BFGS and Partially Separable Quasi-Newton

Below we show the ratio of flops required in the matrix-vector product in 
a BFGS method relative to the flops required in the matrix-vector product 
for the conjugate gradient method for the quasi-Newton method:

```{r compare_flops}
vals <- expand.grid(n = 2^(8:13), p = 2^(2:4), q = 2^(2:8))
vals <- within(vals, {
  flops_qsn <- 2L * n * (p + q) * (p + q + 1L)
  flops_bfgs <- 2L * (q * n + p)^2
  ratio <- flops_bfgs / flops_qsn
})
nq <- length(unique(vals$q))
tvals <- c(vals$n[seq_len(NROW(vals) / nq)], 
           vals$p[seq_len(NROW(vals) / nq)], floor(vals[, "ratio"]))

vals <- matrix(
  tvals, ncol = nq + 2L, dimnames = list(
    NULL, c("n", "p/q", unique(vals$q))))
knitr::kable(vals)
```

## References
